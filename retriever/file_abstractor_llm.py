# retriever/file_abstractor_llm.py

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm

LLM_MODEL = "Qwen/Qwen3-4B-Instruct-2507"  # è‡ªè¡Œæ›æ¨¡å‹

class LLMAbstractor:
    def __init__(self, model_name: str = LLM_MODEL):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )

    @torch.no_grad()
    def summarize(self, text: str) -> str:
        prompt = f"è«‹å°‡ä»¥ä¸‹æ•™æå…§å®¹æ¿ƒç¸®æˆé‡é»æ‘˜è¦ï¼ˆè¶Šæ¸…æ¥šè¶Šå¥½ï¼‰ï¼š\n{text}\n\næ‘˜è¦ï¼š"

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.2
        )

        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ---------------------------------------------------------
        # ğŸ”¥ å¾ã€Œæ‘˜è¦ï¼šã€å¾Œé¢é–‹å§‹æˆªå–ï¼ˆå»æ‰ promptï¼‰
        # ---------------------------------------------------------
        if "æ‘˜è¦ï¼š" in decoded:
            summary = decoded.split("æ‘˜è¦ï¼š", 1)[1].strip()
        else:
            # å¦‚æœæ¨¡å‹æ²’æœ‰ç…§æ ¼å¼è¼¸å‡º fallback åˆ°å…¨æ–‡
            summary = decoded.strip()

        return summary



def abstract_chunks(chunks: list) -> list:
    """
    å°æ¯å€‹ chunk åš LLM æ‘˜è¦
    """
    abs_model = LLMAbstractor()
    results = []

    for c in tqdm(chunks, desc="æ‘˜è¦ä¸­"):
        summary = abs_model.summarize(c)
        results.append(summary)

    return results
